[
  {
    "id": "ext-1760837660447",
    "type": "file",
    "name": "puraviba_callbacks.py",
    "content": "# Backend/API/puraviba_callbacks.py\n# Purpose: Implement Pura Viba IDE callbacks to start/stop Docker Compose using a per-project\n#          name and dynamically selected host ports, then return the frontend URL to open.\n# Imports From: Backend/API/utils.py\n# Exported To: Backend/API/project_routes.py\nfrom __future__ import annotations\n\nimport hashlib\nimport os\nimport re\nimport shutil\nimport socket\nimport subprocess\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Tuple\n\nimport utils as u  # IDE-local utilities and logger\n\n\n# --------------------------- logging helper ---------------------------\n\n\ndef _logger():\n    return u.app.logger  # type: ignore[attr-defined]\n\n\n# --------------------------- compose helpers ---------------------------\n\n\ndef _derive_project_name(project_root: Path) -> str:\n    \"\"\"\n    Create a deterministic, compose-compatible project name from the absolute project root.\n    \"\"\"\n    root = str(project_root.resolve())\n    base = project_root.resolve().name.lower()\n    base = re.sub(r\"[^a-z0-9_-]+\", \"-\", base)\n    digest = hashlib.sha1(root.encode(\"utf-8\")).hexdigest()[:8]\n    return f\"pv-{base}-{digest}\"\n\n\ndef _compose_cmd() -> List[str]:\n    \"\"\"\n    Prefer 'docker compose', fall back to legacy 'docker-compose'.\n    \"\"\"\n    try:\n        subprocess.run(\n            [\"docker\", \"compose\", \"version\"],\n            stdout=subprocess.DEVNULL,\n            stderr=subprocess.DEVNULL,\n            check=True,\n        )\n        return [\"docker\", \"compose\"]\n    except Exception:\n        pass\n\n    exe = shutil.which(\"docker-compose\")\n    if exe:\n        return [exe]\n\n    raise RuntimeError(\"Neither 'docker compose' nor 'docker-compose' is available on PATH.\")\n\n\ndef _run(\n    args: List[str],\n    cwd: Path,\n    env: Dict[str, str] | None = None,\n    check: bool = True,\n) -> subprocess.CompletedProcess:\n    _logger().info(\"[callbacks] run: cwd=%s cmd=%s\", str(cwd), \" \".join(args))\n    proc = subprocess.run(\n        args,\n        cwd=str(cwd),\n        env=env,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        text=True,\n    )\n    if check and proc.returncode != 0:\n        _logger().error(\n            \"[callbacks] command failed rc=%s\\nstdout:\\n%s\\nstderr:\\n%s\",\n            proc.returncode,\n            proc.stdout,\n            proc.stderr,\n        )\n        raise RuntimeError(\n            f\"Command failed ({proc.returncode}): {' '.join(args)}\\n{proc.stderr}\"\n        )\n    return proc\n\n\ndef _pick_free_tcp_port() -> int:\n    \"\"\"\n    Ask the OS for a free port on 127.0.0.1. The port is released immediately.\n    \"\"\"\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        s.bind((\"127.0.0.1\", 0))\n        return s.getsockname()[1]\n\n\ndef _compose_up_detached(\n    project_root: Path,\n    compose_cmd: List[str],\n    project_name: str,\n    extra_env: Dict[str, str],\n) -> None:\n    \"\"\"\n    Start the stack in detached mode with a per-project name. Host ports are injected by env.\n    \"\"\"\n    base_env = os.environ.copy()\n    base_env.update(extra_env or {})\n    _run(\n        compose_cmd + [\"-p\", project_name, \"-f\", \"docker-compose.yml\", \"up\", \"-d\"],\n        cwd=project_root,\n        env=base_env,\n        check=True,\n    )\n\n\ndef _parse_compose_port_output(output: str) -> Tuple[str, int]:\n    \"\"\"\n    Parse `docker compose port` output, preferring IPv4 if present.\n    \"\"\"\n    lines = [ln.strip() for ln in output.splitlines() if ln.strip()]\n    if not lines:\n        raise RuntimeError(\"No port mapping found.\")\n    for ln in lines:\n        if re.match(r\"^\\d{1,3}(\\.\\d{1,3}){3}:\\d+$\", ln):\n            host, port = ln.split(\":\")\n            return host, int(port)\n    host, port = lines[0].rsplit(\":\", 1)\n    host = host.strip(\"[]\")\n    return host, int(port)\n\n\ndef _compose_port(\n    project_root: Path, compose_cmd: List[str], project_name: str, service: str, port: int\n) -> Tuple[str, int]:\n    proc = _run(\n        compose_cmd + [\"-p\", project_name, \"port\", service, str(port)],\n        cwd=project_root,\n        check=True,\n    )\n    return _parse_compose_port_output(proc.stdout)\n\n\ndef _result_payload(\n    protocol: str,\n    fe_host: str,\n    fe_port: int,\n    be_host: str,\n    be_port: int,\n    project_name: str,\n) -> Dict[str, Any]:\n    frontend_url = f\"{protocol}://{fe_host}:{fe_port}\"\n    backend_url = f\"{protocol}://{be_host}:{be_port}\"\n    return {\n        \"status\": \"ok\",\n        \"openUrl\": frontend_url,  # preferred by IDEs that auto-open\n        \"url\": frontend_url,  # generic for UIs expecting 'url'\n        \"frontend\": {\n            \"service\": \"frontend\",\n            \"host\": fe_host,\n            \"port\": fe_port,\n            \"url\": frontend_url,\n        },\n        \"backend\": {\n            \"service\": \"backend\",\n            \"host\": be_host,\n            \"port\": be_port,\n            \"url\": backend_url,\n        },\n        \"project\": {\"name\": project_name},\n    }\n\n\n# --------------------------- callbacks ---------------------------\n\n\ndef _compose_launch(project_root: Path, script_cfg: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"\n    Launch docker compose with a per-project name and unique host ports,\n    then return the mapped frontend URL (and backend URL).\n    \"\"\"\n    protocol = (script_cfg.get(\"protocol\") or \"http\").strip()\n    fe_internal = int(script_cfg.get(\"frontend_internal_port\") or 5173)\n    be_internal = int(script_cfg.get(\"backend_internal_port\") or 8000)\n    fe_service = (script_cfg.get(\"frontend_service\") or \"frontend\").strip()\n    be_service = (script_cfg.get(\"backend_service\") or \"backend\").strip()\n\n    compose_cmd = _compose_cmd()\n    project_name = _derive_project_name(project_root)\n\n    # Choose free host ports to avoid collisions across multiple running instances.\n    # Respect existing env if the IDE/user explicitly provided HOST_* values.\n    host_fe = os.environ.get(\"HOST_FRONTEND_PORT\")\n    host_be = os.environ.get(\"HOST_BACKEND_PORT\")\n    fe_host_port = int(host_fe) if (host_fe and host_fe.isdigit()) else _pick_free_tcp_port()\n    be_host_port = int(host_be) if (host_be and host_be.isdigit()) else _pick_free_tcp_port()\n\n    extra_env = {\n        # Satisfies ${UID}, ${GID} substitutions for volume ownership in docker-compose.yml\n        **({k: os.environ[k] for k in (\"UID\", \"GID\") if k in os.environ}),\n        # Drive the host port mappings declared in docker-compose.yml\n        \"HOST_FRONTEND_PORT\": str(fe_host_port),\n        \"HOST_BACKEND_PORT\": str(be_host_port),\n    }\n\n    _logger().info(\n        \"[callbacks] compose_launch: project=%s host_fe=%s host_be=%s\",\n        project_name,\n        fe_host_port,\n        be_host_port,\n    )\n\n    _compose_up_detached(project_root, compose_cmd, project_name, extra_env)\n\n    fe_host, fe_port = _compose_port(\n        project_root, compose_cmd, project_name, fe_service, fe_internal\n    )\n    be_host, be_port = _compose_port(\n        project_root, compose_cmd, project_name, be_service, be_internal\n    )\n\n    _logger().info(\n        \"[callbacks] compose_launch: frontend=%s:%s backend=%s:%s\",\n        fe_host,\n        fe_port,\n        be_host,\n        be_port,\n    )\n    return _result_payload(protocol, fe_host, fe_port, be_host, be_port, project_name)\n\n\ndef _compose_shutdown(project_root: Path) -> Dict[str, Any]:\n    \"\"\"\n    Stop and remove the compose stack created by _compose_launch for this project root.\n    \"\"\"\n    compose_cmd = _compose_cmd()\n    project_name = _derive_project_name(project_root)\n    _logger().info(\"[callbacks] compose_shutdown: project=%s\", project_name)\n    _run(\n        compose_cmd + [\"-p\", project_name, \"down\", \"--volumes\", \"--remove-orphans\"],\n        cwd=project_root,\n        check=True,\n    )\n    return {\"status\": \"ok\", \"message\": f\"Down: {project_name}\"}\n\n\n# --------------------------- dispatcher ---------------------------\n\n\ndef run_callback(name: str, project_root: Path, script_cfg: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"\n    Entry point invoked by the IDE. Dispatches by callback name.\n\n    Known callbacks:\n      - compose_launch: starts the stack and returns {openUrl, url, frontend, backend, project}\n      - compose_shutdown: stops the stack created for this project root\n    \"\"\"\n    project_root = Path(project_root)\n    if name == \"compose_launch\":\n        return _compose_launch(project_root, script_cfg)\n    if name == \"compose_shutdown\":\n        return _compose_shutdown(project_root)\n\n    raise ValueError(f\"Unknown callback: {name}\")\n",
    "status": "loaded"
  },
  {
    "id": "ext-1760837712656",
    "type": "file",
    "name": "project_routes.py",
    "content": "# Backend/API/project_routes.py\n# Purpose: API routes for project data. Adds detailed debug logging to backend.log to help diagnose hidden/dotfile visibility and user vs project showAllFiles settings.\n# Imports From: utils.py, schemas.py, puraviba_callbacks.py\n# Exported To: app.py\nfrom fastapi import APIRouter, Request, HTTPException\nfrom fastapi.responses import JSONResponse\nimport utils as u\nfrom schemas import WriteFilesPayload, WriteFilesResult, FetchUrlPayload\nfrom pathlib import Path\nfrom git import Repo, GitCommandError\nimport subprocess\nimport shlex\nimport requests\nfrom bs4 import BeautifulSoup\nimport io\nfrom pypdf import PdfReader\nfrom puraviba_callbacks import run_callback\nimport os\n\n\nproject_router = APIRouter(prefix=\"/api\")\n\n\n@project_router.get(\"/load_project\")\nasync def load_project(request: Request):\n    root = request.query_params.get(\"root\")\n    show_all = request.query_params.get(\"showAll\", \"false\").lower() == \"true\"\n    if not root:\n        u.app.logger.error(\"Missing 'root' query parameter for loading project settings.\")\n        raise HTTPException(status_code=400, detail=\"Missing 'root' query parameter.\")\n\n    u.app.logger.info(\"[load_project] Request params: root=%s, showAll=%s\", root, show_all)\n\n    project_root = Path(root)\n    if not project_root.is_dir():\n        u.app.logger.error(f\"Project root is not a valid directory: {root}\")\n        raise HTTPException(\n            status_code=400, detail=f\"Project root is not a valid directory: {root}\"\n        )\n\n    project_settings_path = project_root / \".puraviba\" / \"project.json\"\n    u.app.logger.info(f\"Loading project settings from: {project_settings_path}\")\n\n    default_project_settings = {\n        \"hiddenFiles\": [],\n        \"focusedFiles\": [],\n        \"commitMessage\": \"\",\n        \"description\": \"\",\n        \"selectedExternalSources\": [],\n        \"showAllFiles\": False,\n    }\n\n    project_settings = u._safe_json_load(\n        project_settings_path, default_project_settings.copy()\n    )\n\n    # Extra diagnostics: what's in the root directory (especially dotfiles)?\n    try:\n        entries = sorted(os.listdir(project_root))\n        dotfiles = [e for e in entries if e.startswith(\".\")]\n        u.app.logger.info(\n            \"[load_project] Root '%s' has %d entries, %d dotfiles: %s\",\n            str(project_root),\n            len(entries),\n            len(dotfiles),\n            \", \".join(dotfiles[:25]),\n        )\n    except Exception as e:\n        u.app.logger.warning(\n            f\"[load_project] Failed listing project root '{project_root}': {e}\"\n        )\n\n    # Also log user-level showAllFiles for correlation\n    user_settings = u._safe_json_load(\n        u.USER_SETTINGS_JSON_PATH, u.DEFAULT_USER_SETTINGS_STRUCTURE.copy()\n    )\n    user_show_all = bool(user_settings.get(\"showAllFiles\", False))\n    project_show_all = bool(project_settings.get(\"showAllFiles\", False))\n    u.app.logger.info(\n        \"[load_project] User showAllFiles=%s, Project showAllFiles=%s\",\n        user_show_all,\n        project_show_all,\n    )\n\n    rules_path = project_root / \".puraviba\" / \"rules.json\"\n    rules_data = u._safe_json_load(rules_path, [])\n\n    scripts_path = project_root / \".puraviba\" / \"scripts.json\"\n    scripts_data = u._safe_json_load(scripts_path, {})\n\n    error_handler_path = project_root / \".puraviba\" / \"error_handler.json\"\n    error_handler_defaults = {\"auto\": True, \"errorFiles\": [], \"fix_attempts\": 1}\n    error_handler_data = u._safe_json_load(error_handler_path, error_handler_defaults)\n\n    external_sources_path = project_root / \".puraviba\" / \"external_sources.json\"\n    external_sources_data = u._safe_json_load(external_sources_path, [])\n\n    response_data = {\n        \"hiddenFiles\": project_settings.get(\"hiddenFiles\", []),\n        \"focusedFiles\": project_settings.get(\"focusedFiles\", []),\n        \"commitMessage\": project_settings.get(\"commitMessage\", \"\"),\n        \"description\": project_settings.get(\"description\", \"\"),\n        \"selectedExternalSources\": project_settings.get(\"selectedExternalSources\", []),\n        \"rules\": rules_data,\n        \"scripts\": scripts_data,\n        \"errorHandlerSettings\": error_handler_data,\n        \"externalSources\": external_sources_data,\n        \"showAllFiles\": bool(project_settings.get(\"showAllFiles\", False)),\n    }\n    u.app.logger.info(\n        f\"Returning settings for {root}: hiddenFiles={len(response_data['hiddenFiles'])}, \"\n        f\"focusedFiles={len(response_data['focusedFiles'])}, showAllFiles={response_data['showAllFiles']}\"\n    )\n    return JSONResponse(response_data)\n\n\n@project_router.post(\"/save_project\")\nasync def save_project(request: Request):\n    payload = await request.json()\n    if (\n        not isinstance(payload, dict)\n        or \"activeProjectRoot\" not in payload\n        or \"projectData\" not in payload\n    ):\n        u.app.logger.error(\"Invalid payload received for saving project settings.\")\n        raise HTTPException(\n            status_code=400,\n            detail=\"Invalid payload. Requires 'activeProjectRoot' and 'projectData'.\",\n        )\n\n    root = payload[\"activeProjectRoot\"]\n    project_data = payload[\"projectData\"]\n\n    project_root = Path(root)\n    if not project_root.is_dir():\n        u.app.logger.error(f\"Project root is not a valid directory: {root}\")\n        raise HTTPException(\n            status_code=400, detail=f\"Project root is not a valid directory: {root}\"\n        )\n\n    project_settings_path = project_root / \".puraviba\" / \"project.json\"\n    u.app.logger.info(f\"Saving project settings to: {project_settings_path}\")\n\n    data_to_save = {\n        \"hiddenFiles\": project_data.get(\"hiddenFiles\", []),\n        \"focusedFiles\": project_data.get(\"focusedFiles\", []),\n        \"commitMessage\": project_data.get(\"commitMessage\", \"\"),\n        \"description\": project_data.get(\"description\", \"\"),\n        \"selectedExternalSources\": project_data.get(\"selectedExternalSources\", []),\n        \"showAllFiles\": bool(project_data.get(\"showAllFiles\", False)),\n    }\n\n    u._safe_json_write(project_settings_path, data_to_save)\n    return JSONResponse({\"message\": f\"Project settings saved successfully for {root}\"})\n\n\n@project_router.post(\"/save_error_handler_settings\")\nasync def save_error_handler_settings(request: Request):\n    payload = await request.json()\n    root = payload.get(\"activeProjectRoot\")\n    settings = payload.get(\"settings\")\n\n    if not root or settings is None:\n        raise HTTPException(\n            status_code=400,\n            detail=\"Missing 'activeProjectRoot' or 'settings' in payload.\",\n        )\n\n    project_root = Path(root)\n    if not project_root.is_dir():\n        raise HTTPException(\n            status_code=400, detail=f\"Project root is not a valid directory: {root}\"\n        )\n\n    settings_path = project_root / \".puraviba\" / \"error_handler.json\"\n    u.app.logger.info(f\"Saving error handler settings to: {settings_path}\")\n    u._safe_json_write(settings_path, settings)\n    return JSONResponse({\"message\": \"Error handler settings saved successfully\"})\n\n\n@project_router.post(\"/save_external_sources\")\nasync def save_external_sources(request: Request):\n    payload = await request.json()\n    root = payload.get(\"activeProjectRoot\")\n    sources_data = payload.get(\"externalSources\")\n\n    if not root or sources_data is None:\n        raise HTTPException(\n            status_code=400,\n            detail=\"Missing 'activeProjectRoot' or 'externalSources' in payload.\",\n        )\n\n    project_root = Path(root)\n    if not project_root.is_dir():\n        raise HTTPException(\n            status_code=400, detail=f\"Project root is not a valid directory: {root}\"\n        )\n\n    sources_path = project_root / \".puraviba\" / \"external_sources.json\"\n    u.app.logger.info(f\"Saving external sources to: {sources_path}\")\n    u._safe_json_write(sources_path, sources_data)\n    return JSONResponse({\"message\": \"External sources saved successfully\"})\n\n\n@project_router.post(\"/delete_external_source\")\nasync def delete_external_source(request: Request):\n    payload = await request.json()\n    root = payload.get(\"activeProjectRoot\")\n    source_name = payload.get(\"sourceName\")\n\n    if not root or not source_name:\n        raise HTTPException(\n            status_code=400,\n            detail=\"Missing 'activeProjectRoot' or 'sourceName' in payload.\",\n        )\n\n    project_root = Path(root)\n    if not project_root.is_dir():\n        raise HTTPException(\n            status_code=400, detail=f\"Project root is not a valid directory: {root}\"\n        )\n\n    # The file saved on the backend is the stem of the original filename + .txt\n    text_filename = Path(source_name).stem + \".txt\"\n    externals_dir = project_root / \".puraviba\" / \"externals\"\n    file_to_delete = externals_dir / text_filename\n\n    # Security check to prevent path traversal\n    try:\n        if not file_to_delete.resolve().is_relative_to(externals_dir.resolve()):\n            raise ValueError(\"Path traversal attempt.\")\n    except (ValueError, FileNotFoundError):\n        u.app.logger.warning(\n            f\"Blocked path traversal or invalid path for external source deletion: {file_to_delete}\"\n        )\n        raise HTTPException(status_code=403, detail=\"Path traversal detected.\")\n\n    if file_to_delete.is_file():\n        try:\n            file_to_delete.unlink()\n            u.app.logger.info(f\"Deleted external source file: {file_to_delete}\")\n            return JSONResponse({\"message\": \"External source file deleted successfully.\"})\n        except Exception as e:\n            u.app.logger.error(\n                f\"Failed to delete external source file {file_to_delete}: {e}\"\n            )\n            raise HTTPException(status_code=500, detail=f\"Failed to delete file: {e}\")\n    else:\n        u.app.logger.info(\n            f\"External source file not found for deletion, skipping: {file_to_delete}\"\n        )\n        return JSONResponse({\"message\": \"File not found, no action taken.\"})\n\n\n@project_router.post(\"/save_scripts\")\nasync def save_scripts(request: Request):\n    payload = await request.json()\n    root = payload.get(\"activeProjectRoot\")\n    scripts_data = payload.get(\"scripts\")\n\n    if not root or scripts_data is None:\n        raise HTTPException(\n            status_code=400,\n            detail=\"Missing 'activeProjectRoot' or 'scripts' in payload.\",\n        )\n\n    project_root = Path(root)\n    if not project_root.is_dir():\n        raise HTTPException(\n            status_code=400, detail=f\"Project root is not a valid directory: {root}\"\n        )\n\n    scripts_path = project_root / \".puraviba\" / \"scripts.json\"\n    u.app.logger.info(f\"Saving scripts to: {scripts_path}\")\n    u._safe_json_write(scripts_path, scripts_data)\n    return JSONResponse({\"message\": \"Scripts saved successfully\"})\n\n\n@project_router.post(\"/run_script\")\nasync def run_script(request: Request):\n    payload = await request.json()\n    root = payload.get(\"activeProjectRoot\")\n    script_name = payload.get(\"scriptName\")\n\n    if not root or not script_name:\n        raise HTTPException(\n            status_code=400,\n            detail=\"Missing 'activeProjectRoot' or 'scriptName' in payload.\",\n        )\n\n    project_root = Path(root)\n    if not project_root.is_dir():\n        raise HTTPException(\n            status_code=400, detail=f\"Project root is not a valid directory: {root}\"\n        )\n\n    scripts_path = project_root / \".puraviba\" / \"scripts.json\"\n    scripts = u._safe_json_load(scripts_path, {})\n\n    script_to_run = scripts.get(script_name)\n    if not script_to_run or \"script\" not in script_to_run:\n        raise HTTPException(\n            status_code=404, detail=f\"Script '{script_name}' not found or is invalid.\"\n        )\n\n    script_type = (script_to_run.get(\"type\") or \"standard\").strip().lower()\n    command = script_to_run[\"script\"]\n    relative_path = script_to_run.get(\"path\", \".\")\n    cwd = (project_root / relative_path).resolve()\n\n    if not cwd.is_dir():\n        raise HTTPException(\n            status_code=400, detail=f\"Script path is not a valid directory: {cwd}\"\n        )\n\n    try:\n        cwd.relative_to(project_root.resolve())\n    except ValueError:\n        raise HTTPException(\n            status_code=403, detail=\"Script path is outside the project root.\"\n        )\n\n    if script_type == \"callback\":\n        try:\n            u.app.logger.info(\n                f\"Invoking Pura Viba callback for script '{script_name}': '{command}' in '{cwd}'\"\n            )\n            result = run_callback(command, project_root, script_to_run)\n            return JSONResponse(\n                {\n                    \"message\": f\"Callback '{command}' executed successfully.\",\n                    \"result\": result,\n                }\n            )\n        except ValueError as e:\n            raise HTTPException(status_code=400, detail=str(e))\n        except Exception as e:  # pragma: no cover - defensive logging\n            u.app.logger.error(\n                f\"Failed while executing callback '{command}' for script '{script_name}': {e}\",\n                exc_info=True,\n            )\n            raise HTTPException(status_code=500, detail=f\"Failed to run callback: {e}\")\n\n    try:\n        u.app.logger.info(f\"Running script '{script_name}': `{command}` in `{cwd}`\")\n        proc = subprocess.Popen(\n            shlex.split(command),\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            start_new_session=True,\n        )\n        u.app.logger.info(f\"Started script '{script_name}' with PID: {proc.pid}\")\n        return JSONResponse({\"message\": f\"Script '{script_name}' started successfully.\"})\n    except FileNotFoundError:\n        raise HTTPException(\n            status_code=500, detail=f\"Command not found: {shlex.split(command)[0]}\"\n        )\n    except Exception as e:\n        u.app.logger.error(f\"Failed to run script '{script_name}': {e}\", exc_info=True)\n        raise HTTPException(status_code=500, detail=f\"Failed to run script: {e}\")\n\n\n@project_router.post(\"/process_pdf\")\nasync def process_pdf(request: Request):\n    root = request.query_params.get(\"root\")\n    filename = request.query_params.get(\"filename\")\n    if not root or not filename:\n        u.app.logger.error(\n            \"Missing 'root' or 'filename' query parameter for PDF processing.\"\n        )\n        raise HTTPException(\n            status_code=400, detail=\"Missing 'root' or 'filename' query parameter.\"\n        )\n\n    project_root = Path(root)\n    if not project_root.is_dir():\n        u.app.logger.error(f\"Project root is not a valid directory: {root}\")\n        raise HTTPException(\n            status_code=400, detail=f\"Project root is not a valid directory: {root}\"\n        )\n\n    pdf_data = await request.body()\n    if not pdf_data:\n        u.app.logger.error(\"No PDF data received for processing.\")\n        raise HTTPException(status_code=400, detail=\"No PDF data received.\")\n\n    try:\n        pdf_stream = io.BytesIO(pdf_data)\n        reader = PdfReader(pdf_stream)\n        text = \"\"\n        for page in reader.pages:\n            page_text = page.extract_text()\n            if page_text:\n                text += page_text + \"\\n\"\n\n        externals_dir = project_root / \".puraviba\" / \"externals\"\n        externals_dir.mkdir(parents=True, exist_ok=True)\n\n        text_filename = Path(filename).stem + \".txt\"\n        text_filepath = externals_dir / text_filename\n\n        text_filepath.write_text(text, encoding=\"utf-8\")\n        u.app.logger.info(f\"Saved extracted PDF text to {text_filepath}\")\n\n        return JSONResponse({\"content\": text})\n\n    except Exception as e:\n        u.app.logger.error(f\"Failed to process PDF '{filename}': {e}\", exc_info=True)\n        raise HTTPException(status_code=500, detail=f\"Failed to process PDF: {str(e)}\")\n\n\n@project_router.post(\"/fetch_url\")\nasync def fetch_url(payload: FetchUrlPayload):\n    try:\n        headers = {\n            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\"\n        }\n        response = requests.get(payload.url, timeout=10, headers=headers)\n        response.raise_for_status()\n\n        content_type = response.headers.get(\"Content-Type\", \"\").lower()\n\n        if \"html\" in content_type:\n            soup = BeautifulSoup(response.text, \"html.parser\")\n            for script_or_style in soup([\"script\", \"style\"]):\n                script_or_style.decompose()\n            text = soup.get_text()\n            lines = (line.strip() for line in text.splitlines())\n            chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n            text = \"\\n\".join(chunk for chunk in chunks if chunk)\n            return JSONResponse({\"content\": text})\n        else:\n            return JSONResponse({\"content\": response.text})\n\n    except requests.exceptions.RequestException as e:\n        u.app.logger.error(f\"Failed to fetch URL '{payload.url}': {e}\")\n        raise HTTPException(status_code=400, detail=f\"Failed to fetch URL: {e}\")\n    except Exception as e:\n        u.app.logger.error(\n            f\"An unexpected error occurred while fetching URL '{payload.url}': {e}\"\n        )\n        raise HTTPException(status_code=500, detail=\"An unexpected error occurred.\")\n\n\n@project_router.post(\"/write_files\", response_model=WriteFilesResult)\nasync def write_files(payload: WriteFilesPayload):\n    \"\"\"\n    Executes a list of file operations within the project root,\n    creates necessary subdirectories, and optionally commits the changes.\n    \"\"\"\n    root = Path(payload.activeProjectRoot)\n    u.app.logger.info(f\"Root : {root}\")\n    if not root.is_dir():\n        raise HTTPException(\n            status_code=400, detail=f\"activeProjectRoot is not a valid directory: {root}\"\n        )\n\n    wrote_paths = []\n    skipped_paths = {}\n\n    for op in payload.operations:\n        rel_path_str = op.file_path\n        content = op.content\n\n        if op.operation != \"write\":\n            u.app.logger.warning(\n                f\"Skipping unsupported operation '{op.operation}' for file '{rel_path_str}'\"\n            )\n            skipped_paths[rel_path_str] = f\"Unsupported operation: {op.operation}\"\n            continue\n\n        try:\n            abs_path = (root / rel_path_str).resolve()\n            u.app.logger.info(f\"Rel Path : {rel_path_str}\")\n            u.app.logger.info(f\"Abs Path : {abs_path}\")\n            if not str(abs_path).startswith(str(root.resolve())):\n                u.app.logger.warning(\n                    f\"Skipping file write due to path traversal attempt: '{rel_path_str}'\"\n                )\n                skipped_paths[rel_path_str] = \"Path traversal detected\"\n                continue\n\n            abs_path.parent.mkdir(parents=True, exist_ok=True)\n            abs_path.write_text(content, encoding=\"utf-8\")\n            wrote_paths.append(rel_path_str)\n            u.app.logger.info(f\"Wrote file: {abs_path}\")\n\n        except Exception as e:\n            u.app.logger.error(\n                f\"Failed to write file '{rel_path_str}': {e}\", exc_info=True\n            )\n            skipped_paths[rel_path_str] = str(e)\n\n    committed = False\n    commit_message = payload.commitMessage.strip()\n    if commit_message and wrote_paths:\n        try:\n            repo = Repo(root)\n            paths_to_add = [(root / p).as_posix() for p in wrote_paths]\n            repo.index.add(paths_to_add)\n\n            if repo.is_dirty(index=True, working_tree=False):\n                repo.index.commit(commit_message)\n                committed = True\n                u.app.logger.info(\n                    f\"Committed {len(wrote_paths)} files with message: '{commit_message}'\"\n                )\n            else:\n                u.app.logger.info(\"No changes to commit after writing files.\")\n\n        except GitCommandError as e:\n            u.app.logger.error(f\"Git commit failed: {e}\", exc_info=True)\n            for p in wrote_paths:\n                if p not in skipped_paths:\n                    skipped_paths[p] = \"File written but commit failed.\"\n        except Exception as e:\n            u.app.logger.error(\n                f\"An unexpected error occurred during commit: {e}\", exc_info=True\n            )\n\n    if committed:\n        try:\n            error_handler_path = root / \".puraviba\" / \"error_handler.json\"\n            if error_handler_path.exists():\n                settings = u._safe_json_load(error_handler_path, {})\n                error_files = settings.get(\"errorFiles\", [])\n                if error_files:\n                    u.app.logger.info(\n                        f\"Clearing {len(error_files)} error log files post-commit.\"\n                    )\n                    for rel_path in error_files:\n                        abs_path = (root / rel_path).resolve()\n                        if str(abs_path).startswith(str(root.resolve())):\n                            if abs_path.is_file():\n                                try:\n                                    abs_path.write_text(\"\", encoding=\"utf-8\")\n                                    u.app.logger.info(f\"Cleared error log: {abs_path}\")\n                                except Exception as e:\n                                    u.app.logger.error(\n                                        f\"Failed to clear error log {abs_path}: {e}\"\n                                    )\n                            else:\n                                u.app.logger.warning(\n                                    f\"Error log file not found or not a file, skipping: {abs_path}\"\n                                )\n                        else:\n                            u.app.logger.warning(\n                                f\"Skipping error log clear due to path traversal attempt: '{rel_path}'\"\n                            )\n        except Exception as e:\n            u.app.logger.error(\n                f\"An unexpected error occurred during error log clearing: {e}\",\n                exc_info=True,\n            )\n\n    return WriteFilesResult(\n        wrote=wrote_paths,\n        skipped=skipped_paths,\n        committed=committed,\n        commitMessage=commit_message if committed else None,\n    )\n\n\n@project_router.post(\"/save_last_active\")\nasync def save_last_active(request: Request):\n    payload = await request.json()\n    if not isinstance(payload, dict) or \"lastActiveProjectRoot\" not in payload:\n        u.app.logger.error(\n            \"Invalid payload received for saving last active project root.\"\n        )\n        raise HTTPException(\n            status_code=400, detail=\"Invalid payload. Requires 'lastActiveProjectRoot'.\"\n        )\n\n    last_active = payload[\"lastActiveProjectRoot\"]\n    u.app.logger.info(f\"Saving last active project root to user settings: {last_active}\")\n\n    user_settings = u._safe_json_load(\n        u.USER_SETTINGS_JSON_PATH, u.DEFAULT_USER_SETTINGS_STRUCTURE.copy()\n    )\n\n    user_settings[\"lastActiveProjectRoot\"] = last_active\n    u._safe_json_write(u.USER_SETTINGS_JSON_PATH, user_settings)\n    return JSONResponse({\"message\": \"Last active project root saved successfully\"})\n\n\n@project_router.get(\"/load_user_settings\")\nasync def load_user_settings():\n    u.app.logger.info(\"Loading user settings from dedicated file\")\n    user_settings = u._safe_json_load(\n        u.USER_SETTINGS_JSON_PATH, u.DEFAULT_USER_SETTINGS_STRUCTURE.copy()\n    )\n    u.app.logger.info(\n        \"Returning user settings. showAllFiles=%s lastActiveProjectRoot=%s\",\n        bool(user_settings.get(\"showAllFiles\", False)),\n        user_settings.get(\"lastActiveProjectRoot\"),\n    )\n    return JSONResponse(user_settings)\n\n\n@project_router.post(\"/save_user_settings\")\nasync def save_user_settings(request: Request):\n    settings_payload = await request.json()\n    if not isinstance(settings_payload, dict):\n        u.app.logger.error(\"Invalid payload received for saving user settings.\")\n        raise HTTPException(\n            status_code=400, detail=\"Invalid payload. Must be a JSON object.\"\n        )\n\n    u.app.logger.info(\n        \"Saving user settings to dedicated file (showAllFiles=%s, lastActive=%s)\",\n        bool(settings_payload.get(\"showAllFiles\", False)),\n        settings_payload.get(\"lastActiveProjectRoot\"),\n    )\n    u._safe_json_write(u.USER_SETTINGS_JSON_PATH, settings_payload)\n    return JSONResponse({\"message\": \"User settings saved successfully\"})\n",
    "status": "loaded"
  }
]